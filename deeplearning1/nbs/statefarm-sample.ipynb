{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter State Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2018-07-26T14:57:32.653811Z",
     "start_time": "2018-07-26T14:57:30.440200Z"
=======
     "end_time": "2018-07-22T23:37:51.360530Z",
     "start_time": "2018-07-22T23:37:51.345539Z"
>>>>>>> f0996f6c4033e734ce389f9c35f6eb142edc0892
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "import imp\n",
    "import utils\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2018-07-26T14:57:33.789447Z",
     "start_time": "2018-07-26T14:57:33.785450Z"
=======
     "end_time": "2018-07-22T23:37:54.566702Z",
     "start_time": "2018-07-22T23:37:54.562704Z"
>>>>>>> f0996f6c4033e734ce389f9c35f6eb142edc0892
    }
   },
   "outputs": [],
   "source": [
    "batch_size=64  # 最好的batch_size 是64，但是gtx970不行，只能这样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 建立样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following assumes you've already created your validation set - remember that the training and validation set should contain *different drivers*, as mentioned on the Kaggle competition page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:37:57.881811Z",
     "start_time": "2018-07-22T23:37:57.876814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML\\\\courses\\\\deeplearning1\\\\nbs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:38:01.985471Z",
     "start_time": "2018-07-22T23:38:01.972479Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ML\\courses\\deeplearning1\\nbs\\data\\state\n"
     ]
    }
   ],
   "source": [
    "%cd data/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:42:06.083488Z",
     "start_time": "2018-07-22T23:42:06.074491Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ML\\courses\\deeplearning1\\nbs\\data\\state\\train\n"
     ]
    }
   ],
   "source": [
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T08:26:10.725655Z",
     "start_time": "2018-07-20T08:26:10.677673Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %mkdir ../sample\n",
    "# %mkdir ../sample/train\n",
    "# %mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:11.594517Z",
     "start_time": "2018-07-22T23:43:11.577544Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in glob('c?'):\n",
    "    os.mkdir('../valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:17.475179Z",
     "start_time": "2018-07-22T23:43:17.449178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d in glob('c?'):\n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    os.mkdir('../sample/valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:20.739304Z",
     "start_time": "2018-07-22T23:43:20.736303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile,move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:23.185905Z",
     "start_time": "2018-07-22T23:43:23.181908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML\\\\courses\\\\deeplearning1\\\\nbs\\\\data\\\\state\\\\train'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:27.249587Z",
     "start_time": "2018-07-22T23:43:27.091693Z"
    }
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:30.365810Z",
     "start_time": "2018-07-22T23:43:30.353817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c5\\\\img_92865.jpg', 'c5\\\\img_16742.jpg', 'c3\\\\img_78179.jpg', ..., 'c8\\\\img_36908.jpg',\n",
       "       'c6\\\\img_3488.jpg', 'c4\\\\img_42713.jpg'], dtype='<U17')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:39.795430Z",
     "start_time": "2018-07-22T23:43:36.177497Z"
    }
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1500): \n",
    "    move(shuf[i], '../valid/' + shuf[i]) # 创建valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:49.140167Z",
     "start_time": "2018-07-22T23:43:45.599152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1500): \n",
    "    copyfile(shuf[i], '../sample/train/' + shuf[i]) # 够方便的办法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:52.169404Z",
     "start_time": "2018-07-22T23:43:52.154413Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ML\\courses\\deeplearning1\\nbs\\data\\state\\valid\n"
     ]
    }
   ],
   "source": [
    "%cd ../valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:43:55.488512Z",
     "start_time": "2018-07-22T23:43:55.485511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML\\\\courses\\\\deeplearning1\\\\nbs\\\\data\\\\state\\\\valid'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-22T23:44:02.769360Z",
     "start_time": "2018-07-22T23:44:00.364730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1000): \n",
    "    copyfile(shuf[i], '../sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T08:57:19.180583Z",
     "start_time": "2018-07-20T08:57:19.170586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T08:57:24.434010Z",
     "start_time": "2018-07-20T08:57:24.413022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%mkdir data/state/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%mkdir data/state/sample/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T14:39:18.271330Z",
     "start_time": "2018-07-20T14:39:18.260320Z"
    }
   },
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T14:57:37.702286Z",
     "start_time": "2018-07-26T14:57:37.383384Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T14:57:39.023849Z",
     "start_time": "2018-07-26T14:57:39.017851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T14:57:40.015533Z",
     "start_time": "2018-07-26T14:57:40.010533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c0\\\\img_10003.jpg',\n",
       " 'c0\\\\img_101159.jpg',\n",
       " 'c0\\\\img_10314.jpg',\n",
       " 'c0\\\\img_11863.jpg',\n",
       " 'c0\\\\img_12238.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T14:57:41.506055Z",
     "start_time": "2018-07-26T14:57:41.191155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# 验证集类别，训练类别，验证标签onehot，训练标签onehot，验证文件名，训练文件名，测试文件名\n",
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames,\n",
    "    test_filename) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 基本模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we try the simplest model and use default parameters. Note the trick of making the first layer a batchnorm layer - that way we don't have to worry about normalizing the input ourselves.\n",
    "\n",
    "首先，我们尝试最简单的模型并使用默认参数。注意使第一个层成为batchnorm层的技巧——这样我们就不用担心自己的输入是否正常化了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T14:58:33.373521Z",
     "start_time": "2018-07-26T14:57:43.595383Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "       BatchNormalization(axis=1,input_shape=(3,224,224)), # 这里必须用input_shape=()\n",
    "       Flatten(),\n",
    "       Dense(10,activation='softmax')\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see below, this training is going nowhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:31:09.193405Z",
     "start_time": "2018-07-20T15:30:49.112996Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "可以看到线性模型，效果很差\n",
    "\n",
    "让我们首先检查一下参数的数量，看看是否有足够的参数来找到一些有用的关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:31:11.232633Z",
     "start_time": "2018-07-20T15:31:11.222995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "超过150万个参数，这应该足够了。顺便说一下，我们需要检查一下为什么这是这一层的参数数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:31:14.231801Z",
     "start_time": "2018-07-20T15:31:14.214243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "10*3*224*224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we have a simple model with no regularization and plenty of parameters, it seems most likely that our learning rate is too high. Perhaps it is jumping to a solution where it predicts one or two classes with high confidence, so that it can give a zero prediction to as many classes as possible - that's the best approach for a model that is no better than random, and there is likely to be where we would end up with a high learning rate. So let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:33:58.627466Z",
     "start_time": "2018-07-20T15:33:52.545021Z"
    }
   },
   "outputs": [],
   "source": [
    "np.round(model.predict_generator(batches)[:10],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our hypothesis was correct. It's nearly always predicting class 1 or 6, with very high confidence. So let's try a lower learning rate:\n",
    "\n",
    "我们的假设是正确的。它几乎总是预测类3，非常有信心。让我们尝试一个更低的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:36:10.656043Z",
     "start_time": "2018-07-20T15:35:50.868870Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great - we found our way out of that hole... Now we can increase the learning rate and see where we can get to.\n",
    "\n",
    "太好了，我们找到了办法……现在我们可以提高学习速度，看看能达到什么程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:36:38.234595Z",
     "start_time": "2018-07-20T15:36:38.214587Z"
    }
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:37:43.567155Z",
     "start_time": "2018-07-20T15:37:05.194169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=4, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're stabilizing at validation accuracy of 0.39. Not great, but a lot better than random. Before moving on, let's check that our validation set on the sample is large enough that it gives consistent results:\n",
    "\n",
    "\n",
    "我们的验证精度稳定在0.71。不是很好，但比随机要好多了。在继续之前，让我们检查一下我们在示例上的验证集是否足够大，从而给出一致的结果:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:38:04.249587Z",
     "start_time": "2018-07-20T15:38:04.112777Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnd_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:40:09.618563Z",
     "start_time": "2018-07-20T15:39:28.574862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_res = [model.evaluate_generator(rnd_batches) for i in range(10)] # 第一列val_loss,第二列是准确度\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:40:28.274879Z",
     "start_time": "2018-07-20T15:40:28.261398Z"
    }
   },
   "outputs": [],
   "source": [
    "val_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yup, pretty consistent - if we see improvements of 3% or more, it's probably not random, based on the above samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The previous model is over-fitting a lot, but we can't use dropout since we only have one layer. We can try to decrease overfitting in our model by adding [l2 regularization](http://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html/2) (i.e. add the sum of squares of the weights to our loss function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:45:06.093990Z",
     "start_time": "2018-07-20T15:44:46.218753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax',kernel_regularizer=regularizers.l2(l=0.01))\n",
    "    ])\n",
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:45:09.212971Z",
     "start_time": "2018-07-20T15:45:09.198021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:45:29.586959Z",
     "start_time": "2018-07-20T15:45:10.071193Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like we can get a bit over 50% accuracy this way. This will be a good benchmark for our future models - if we can't beat 50%, then we're not even beating a linear model trained on a sample, so we'll know that's not a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Single hidden layer 单个隐藏层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next simplest model is to add a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:47:38.668659Z",
     "start_time": "2018-07-20T15:46:30.852482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)\n",
    "\n",
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=5, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not looking very encouraging... which isn't surprising since we know that CNNs are a much better choice for computer vision problems. So we'll try one.\n",
    "\n",
    "因为我们知道CNNs是一个更好的计算机视觉问题的选择。所以我们会尝试一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Single conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2 conv layers with max pooling followed by a simple dense network is a good simple CNN to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:17:30.633432Z",
     "start_time": "2018-07-21T11:17:30.265553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_17 (Batc (None, 3, 224, 224)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 222, 222)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32, 222, 222)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 32, 74, 74)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 72, 72)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64, 72, 72)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 64, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               7373000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 7,395,598\n",
      "Trainable params: 7,395,000\n",
      "Non-trainable params: 598\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# keras 1.1写法要注意，这在keras2中完全不同\n",
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Conv2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Conv2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:30:39.391223Z",
     "start_time": "2018-07-21T11:30:38.908392Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_21 (Batc (None, 3, 224, 224)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 222, 222)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 222, 222)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 32, 74, 74)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 72, 72)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64, 72, 72)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               7373000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 7,395,598\n",
      "Trainable params: 7,395,000\n",
      "Non-trainable params: 598\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 正确的keras2.0写法\n",
    "model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Conv2D(filters=32,kernel_size=(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Conv2D(filters=64,kernel_size=(3,3) , activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:31:12.028045Z",
     "start_time": "2018-07-21T11:31:11.982070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Conv2D(filters=32,kernel_size=(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Conv2D(filters=64,kernel_size=(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D(pool_size=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=2, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)\n",
    "\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=4, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:33:38.279564Z",
     "start_time": "2018-07-21T11:31:14.356318Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 25s 1s/step - loss: 2.4848 - acc: 0.2163 - val_loss: 2.4693 - val_acc: 0.3521\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 25s 1s/step - loss: 1.7315 - acc: 0.4185 - val_loss: 1.7165 - val_acc: 0.4552\n",
      "Epoch 1/4\n",
      "23/23 [==============================] - 25s 1s/step - loss: 1.5028 - acc: 0.5039 - val_loss: 1.5984 - val_acc: 0.4854\n",
      "Epoch 2/4\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.3750 - acc: 0.5459 - val_loss: 1.1363 - val_acc: 0.6188\n",
      "Epoch 3/4\n",
      "23/23 [==============================] - 23s 993ms/step - loss: 1.1797 - acc: 0.6212 - val_loss: 1.2184 - val_acc: 0.6156\n",
      "Epoch 4/4\n",
      "23/23 [==============================] - 22s 960ms/step - loss: 1.1290 - acc: 0.6438 - val_loss: 1.3114 - val_acc: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x238fd4a5e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The training set here is very rapidly reaching a very high accuracy. So if we could regularize this, perhaps we could get a reasonable result.\n",
    "\n",
    "So, what kind of regularization should we try first? As we discussed in lesson 3, we should start with data augmentation.\n",
    "\n",
    "这里的训练集非常迅速地达到非常高的准确度。因此，如果我们能使它合法化，也许我们能得到一个合理的结果。\n",
    "\n",
    "那么，我们首先应该尝试怎样的正规化呢?正如我们在第3课中讨论的，我们应该从数据增强开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To find the best data augmentation parameters, we can try each type of data augmentation, one at a time. For each type, we can try four very different levels of augmentation, and see which is the best. In the steps below we've only kept the single best result we found. We're using the CNN we defined above, since we have already observed it can model the data quickly and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Width shift: move the image left and right -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:52:17.236903Z",
     "start_time": "2018-07-20T15:52:16.986550Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:54:38.039582Z",
     "start_time": "2018-07-20T15:52:20.774996Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Height shift: move the image up and down -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:54:47.577524Z",
     "start_time": "2018-07-20T15:54:47.350603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T15:57:08.563687Z",
     "start_time": "2018-07-20T15:54:50.542574Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random shear angles (max in radians) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rotation: max in degrees -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Channel shift: randomly changing the R,G,B colors - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And finally, putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:37:19.055708Z",
     "start_time": "2018-07-21T11:37:18.832789Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:40:07.180816Z",
     "start_time": "2018-07-21T11:37:45.794532Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 26s 1s/step - loss: 2.5705 - acc: 0.2210 - val_loss: 2.5829 - val_acc: 0.3104\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.7808 - acc: 0.3980 - val_loss: 1.8355 - val_acc: 0.4292\n",
      "Epoch 1/4\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.4958 - acc: 0.4938 - val_loss: 1.4573 - val_acc: 0.5177\n",
      "Epoch 2/4\n",
      "23/23 [==============================] - 23s 997ms/step - loss: 1.3189 - acc: 0.5657 - val_loss: 1.7161 - val_acc: 0.4938\n",
      "Epoch 3/4\n",
      "23/23 [==============================] - 22s 968ms/step - loss: 1.1843 - acc: 0.6035 - val_loss: 1.6608 - val_acc: 0.5125\n",
      "Epoch 4/4\n",
      "23/23 [==============================] - 22s 944ms/step - loss: 1.1443 - acc: 0.6210 - val_loss: 1.5117 - val_acc: 0.5708\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At first glance, this isn't looking encouraging, since the validation set is poor and getting worse. But the training set is getting better, and still has a long way to go in accuracy - so we should try annealing our learning rate and running more epochs, before we make a decisions.\n",
    "\n",
    "乍一看，这并不令人鼓舞，因为验证集很差，而且越来越糟。但是训练集正在变得越来越好，而且在准确性上还有很长的路要走——所以我们应该在做决定之前试着降低我们的学习速度，跑得更有意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:45:16.316896Z",
     "start_time": "2018-07-21T11:43:21.507104Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23/23 [==============================] - 24s 1s/step - loss: 1.0144 - acc: 0.6734 - val_loss: 1.3782 - val_acc: 0.5531\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 23s 1s/step - loss: 0.9734 - acc: 0.6888 - val_loss: 1.0929 - val_acc: 0.6479\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 23s 978ms/step - loss: 0.9168 - acc: 0.7053 - val_loss: 0.6975 - val_acc: 0.7552\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 22s 965ms/step - loss: 0.8277 - acc: 0.7223 - val_loss: 0.5682 - val_acc: 0.8115\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 23s 1s/step - loss: 0.8208 - acc: 0.7310 - val_loss: 0.7728 - val_acc: 0.7250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23904c6ec18>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=5, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lucky we tried that - we starting to make progress! Let's keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T11:54:25.493380Z",
     "start_time": "2018-07-21T11:45:49.152697Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "23/23 [==============================] - 24s 1s/step - loss: 0.7457 - acc: 0.7735 - val_loss: 0.7265 - val_acc: 0.7469\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - 23s 1s/step - loss: 0.7450 - acc: 0.7504 - val_loss: 0.8867 - val_acc: 0.7302\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - 22s 972ms/step - loss: 0.6907 - acc: 0.7738 - val_loss: 0.6827 - val_acc: 0.7667\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - 22s 942ms/step - loss: 0.6660 - acc: 0.7813 - val_loss: 0.5187 - val_acc: 0.8344\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - 21s 913ms/step - loss: 0.6064 - acc: 0.8130 - val_loss: 0.4464 - val_acc: 0.8448\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - 21s 921ms/step - loss: 0.5818 - acc: 0.8175 - val_loss: 0.4846 - val_acc: 0.8479\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - 23s 980ms/step - loss: 0.5181 - acc: 0.8472 - val_loss: 0.5895 - val_acc: 0.7937\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - 20s 891ms/step - loss: 0.5543 - acc: 0.8346 - val_loss: 0.5964 - val_acc: 0.8031\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - 21s 902ms/step - loss: 0.5356 - acc: 0.8336 - val_loss: 0.6148 - val_acc: 0.7792\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - 20s 854ms/step - loss: 0.5028 - acc: 0.8412 - val_loss: 0.4661 - val_acc: 0.8479\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - 21s 913ms/step - loss: 0.4795 - acc: 0.8539 - val_loss: 0.4366 - val_acc: 0.8583\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - 20s 875ms/step - loss: 0.4598 - acc: 0.8669 - val_loss: 0.4914 - val_acc: 0.8469\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - 20s 869ms/step - loss: 0.4447 - acc: 0.8627 - val_loss: 0.3757 - val_acc: 0.8740\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - 19s 841ms/step - loss: 0.4261 - acc: 0.8652 - val_loss: 0.4531 - val_acc: 0.8521\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - 20s 891ms/step - loss: 0.4255 - acc: 0.8662 - val_loss: 0.3732 - val_acc: 0.8781\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - 19s 842ms/step - loss: 0.3978 - acc: 0.8743 - val_loss: 0.5238 - val_acc: 0.8260\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - 20s 881ms/step - loss: 0.4147 - acc: 0.8825 - val_loss: 0.4482 - val_acc: 0.8500\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - 19s 839ms/step - loss: 0.3851 - acc: 0.8845 - val_loss: 0.3541 - val_acc: 0.8990\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - 21s 895ms/step - loss: 0.3543 - acc: 0.8933 - val_loss: 0.3328 - val_acc: 0.8979\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - 20s 869ms/step - loss: 0.3641 - acc: 0.8996 - val_loss: 0.3476 - val_acc: 0.8948\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - 20s 865ms/step - loss: 0.3780 - acc: 0.8986 - val_loss: 0.3567 - val_acc: 0.8927\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - 20s 871ms/step - loss: 0.3278 - acc: 0.9036 - val_loss: 0.4392 - val_acc: 0.8656\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - 20s 865ms/step - loss: 0.3100 - acc: 0.9133 - val_loss: 0.4608 - val_acc: 0.8438\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - 20s 860ms/step - loss: 0.2915 - acc: 0.9278 - val_loss: 0.3775 - val_acc: 0.8656\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - 20s 857ms/step - loss: 0.3094 - acc: 0.9076 - val_loss: 0.3434 - val_acc: 0.8812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23904c6eb38>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch=batches.n // batch_size, epochs=25, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Amazingly, using nothing but a small sample, a simple (not pre-trained) model with no dropout, and data augmentation, we're getting results that would get us into the top 50% of the competition! This looks like a great foundation for our futher experiments.\n",
    "\n",
    "To go further, we'll need to use the whole dataset, since dropout and data volumes are very related, so we can't tweak dropout without using all the data.\n",
    "\n",
    "令人惊讶的是，除了一个小样本、一个没有中途退出的简单模型和数据增强，我们得到的结果将使我们进入竞争的前50% !这看起来是我们未来实验的一个很好的基础。\n",
    "\n",
    "更进一步，我们将需要使用整个数据集，因为drop和数据量是非常相关的，所以我们不能在不使用所有数据的情况下对dropout进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
